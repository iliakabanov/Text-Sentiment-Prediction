{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a70d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "Author: Ilia Kabanov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fecf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task: to predict wether product review is positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16c9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from copy import deepcopy\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import joblib\n",
    "import collections\n",
    "import requests \n",
    "from urllib.parse import urlencode \n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d0b3294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  КАТЕГОРИЧЕСКИ НЕ СОВЕТУЮ брать продукцию texet...        0.0\n",
      "1  первый и последний раз приобрел продукт данног...        0.0\n",
      "2  Если Вы например ходите на разничные тусовки и...        0.0\n",
      "3  Ни кому бы не стала советовать покупать этот т...        0.0\n",
      "4  Из явных глюков, которые не устранились обновл...        0.0\n"
     ]
    }
   ],
   "source": [
    "# Read input data\n",
    "train = pd.read_csv('train.csv', sep=',', engine='python')\n",
    "test = pd.read_csv('test.csv', sep=',', engine='python' )\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17be9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id                                             review\n",
      "0  0  До этого был Хонор 10 и пользовался только анд...\n",
      "1  1                                         не советую\n",
      "2  2  Больше года использования не подвел ни разу. М...\n",
      "3  3  Телефон держит заряд 3-4 дня при 15-20 минут р...\n",
      "4  4  Была раньше только нокия, но по работе пришлос...\n"
     ]
    }
   ],
   "source": [
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576a6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines in X_train with empty 'sentiment' and lower all the words\n",
    "train = train[train['sentiment'].notnull()]\n",
    "train.reset_index(inplace=True)\n",
    "train.drop(['index'], axis=1, inplace=True)\n",
    "train.iloc[:, 0] = train.iloc[:, 0].apply(lambda x: x.lower())\n",
    "\n",
    "# Drop lines with no id in X_train, and lower all the words \n",
    "test = test[test['id'].notnull()]\n",
    "test = test[test['id'].apply(lambda x: x.isnumeric())]\n",
    "test.reset_index(inplace=True)\n",
    "test.drop(['index'], axis=1, inplace=True)\n",
    "test.iloc[:, 1] = test.iloc[:, 1].apply(lambda x: x.lower())\n",
    "test_ids = test['id']\n",
    "X_test = test['review']\n",
    "\n",
    "X_train = train.iloc[:, 0]\n",
    "y_train = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec18b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Then let's use lemmatization procedure with both X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d286c6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020833833b7f42c3add8f82ffb67ab91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "morph = MorphAnalyzer()\n",
    "train_lemmatized_descriptions = []\n",
    "for i, description in tqdm(enumerate(X_train.values), total=len(X_train)):\n",
    "    try:\n",
    "        lemmatized_description = [\n",
    "            morph.parse(token)[0].normal_form for token in\n",
    "            re.findall(r'\\w+', description)\n",
    "        ]\n",
    "        train_lemmatized_descriptions.append(lemmatized_description)\n",
    "    except Exception as e:\n",
    "        print(f'Не удалось распарсить description с индексом={i}:')\n",
    "        print(\"descrition:\")\n",
    "        print(description, end='\\n\\n')\n",
    "        print(e, end='\\n\\n')\n",
    "        train_lemmatized_descriptions.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fdf0d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71da8d66361d4007b22d939952ce7036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21747 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "morph = MorphAnalyzer()\n",
    "test_lemmatized_descriptions = []\n",
    "for i, description in tqdm(enumerate(test['review'].values), total=len(test['review'])):\n",
    "    try:\n",
    "        lemmatized_description = [\n",
    "            morph.parse(token)[0].normal_form for token in\n",
    "            re.findall(r'\\w+', description)\n",
    "        ]\n",
    "        test_lemmatized_descriptions.append(lemmatized_description)\n",
    "    except Exception as e:\n",
    "        print(f'Не удалось распарсить description с индексом={i}:')\n",
    "        print(\"descrition:\")\n",
    "        print(description, end='\\n\\n')\n",
    "        print(e, end='\\n\\n')\n",
    "        train_lemmatized_descriptions.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74011c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform list with lemmatized words into dataset with previous form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57824a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(train_lemmatized_descriptions).fillna(value=' ', inplace=False)\n",
    "X_train = X_train.apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81f0087c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(test_lemmatized_descriptions).fillna(value=' ', inplace=False)\n",
    "X_test = X_test.apply(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lets have a look at our lemmatized train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "854c4ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    категорически не советовать брать продукция te...\n",
       "1    первый и последний раз приобрести продукт данн...\n",
       "2    если вы например ходить на разничный тусовка и...\n",
       "3    ни кто бы не стать советовать покупать этот те...\n",
       "4    из явный глюк который не устраниться обновлени...\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1bb213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    до это быть хонора 10 и пользоваться только ан...\n",
       "1    не советовать                                 ...\n",
       "2    большой год использование не подвести ни раз м...\n",
       "3    телефон держать заряд 3 4 день при 15 20 минут...\n",
       "4    быть ранний только нокия но по работа прийтись...\n",
       "dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007cfde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "We got exactly what we need, our dataset are ready for embedding.\n",
    "Let's count tf_idf of every word in text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd4c6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing td-idf and getting our X_train, y_train\n",
    "russian_stop_words = list(\n",
    "    pd.read_csv('https://raw.githubusercontent.com/stopwords-iso/stopwords-ru/master/stopwords-ru.txt', header=None)[0]\n",
    ")\n",
    "text_corpus = pd.concat([X_train, X_test], axis=0).reset_index().drop(['index'], axis=1, inplace=False).iloc[:, 0]\n",
    "tf_idf = TfidfVectorizer(stop_words=russian_stop_words,\n",
    "                             ngram_range=(1, 2),\n",
    "                             min_df=5,\n",
    "                             max_df=0.99\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "befe8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr: 22,892 x 21,901\n",
      "X_te: 21,747 x 21,901\n",
      "CPU times: total: 6.28 s\n",
      "Wall time: 6.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_idf.fit([' '.join(str(word) for word in sentence) for sentence in train_lemmatized_descriptions])\n",
    "X_tr = tf_idf.transform(\n",
    "    [' '.join(str(word) for word in sentence) for sentence in train_lemmatized_descriptions]\n",
    ")\n",
    "X_te = tf_idf.transform(\n",
    "    [' '.join(str(word) for word in sentence) for sentence in test_lemmatized_descriptions]\n",
    ")\n",
    "print(f\"X_tr: {X_tr.shape[0]:,} x {X_tr.shape[1]:,}\")\n",
    "print(f\"X_te: {X_te.shape[0]:,} x {X_te.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e5fc962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Word2Vec vocabulary, based on our lemmatized X_train\n",
    "text_corpus = train_lemmatized_descriptions\n",
    "d = 50 # embedding dimension\n",
    "w2v_model = Word2Vec(sentences=text_corpus,\n",
    "                     min_count=10,\n",
    "                     window=10,\n",
    "                     vector_size=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74b63f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate words from the dataset into word2vec vectors\n",
    "copy_X_train = deepcopy(X_train)\n",
    "X_train = pd.DataFrame(train_lemmatized_descriptions).fillna(value=0, inplace=False)\n",
    "X_train = X_train.applymap(lambda x: w2v_model.wv[x] if x in w2v_model.wv else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e6bd4e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.044200912, -0.008069042, -0.30456, -0.02325...</td>\n",
       "      <td>[-0.61658484, 0.99378943, 0.669043, -1.5624499...</td>\n",
       "      <td>[-0.38431868, -2.865498, -3.3376913, 1.2617782...</td>\n",
       "      <td>[2.4528542, -1.6575896, -3.5384734, 2.6203744,...</td>\n",
       "      <td>[-0.010254101, -0.28837365, -1.1872854, 0.0344...</td>\n",
       "      <td>[-0.023676293, -0.1382317, 0.0022645644, 0.037...</td>\n",
       "      <td>[-0.55358464, 0.2415753, -0.39307374, 1.591386...</td>\n",
       "      <td>[0.73056346, 1.5591023, -0.77380407, -0.267105...</td>\n",
       "      <td>[-0.55358464, 0.2415753, -0.39307374, 1.591386...</td>\n",
       "      <td>[-0.009174255, 1.0757917, 1.0208045, -0.333414...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.1060523, -0.17561156, -0.5548251, -0.071880...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-0.76158327, -1.7124096, -0.5286558, 0.856061...</td>\n",
       "      <td>[-0.88319707, -1.4063718, 1.593841, 2.488779, ...</td>\n",
       "      <td>[1.1973634, -0.8015441, -2.6771297, 2.8275838,...</td>\n",
       "      <td>[0.18506458, -0.27211612, -1.2946948, 0.298019...</td>\n",
       "      <td>[-0.9703825, 1.7481861, -1.1898727, -1.808933,...</td>\n",
       "      <td>[0.72331065, -0.56463075, -0.323733, -1.384814...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "      <td>[-1.7984205, -1.0761415, 0.80871004, 0.1154940...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 403 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0    \\\n",
       "0  [0.044200912, -0.008069042, -0.30456, -0.02325...   \n",
       "1  [0.1060523, -0.17561156, -0.5548251, -0.071880...   \n",
       "\n",
       "                                                 1    \\\n",
       "0  [-0.61658484, 0.99378943, 0.669043, -1.5624499...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 2    \\\n",
       "0  [-0.38431868, -2.865498, -3.3376913, 1.2617782...   \n",
       "1  [-0.76158327, -1.7124096, -0.5286558, 0.856061...   \n",
       "\n",
       "                                                 3    \\\n",
       "0  [2.4528542, -1.6575896, -3.5384734, 2.6203744,...   \n",
       "1  [-0.88319707, -1.4063718, 1.593841, 2.488779, ...   \n",
       "\n",
       "                                                 4    \\\n",
       "0  [-0.010254101, -0.28837365, -1.1872854, 0.0344...   \n",
       "1  [1.1973634, -0.8015441, -2.6771297, 2.8275838,...   \n",
       "\n",
       "                                                 5    \\\n",
       "0  [-0.023676293, -0.1382317, 0.0022645644, 0.037...   \n",
       "1  [0.18506458, -0.27211612, -1.2946948, 0.298019...   \n",
       "\n",
       "                                                 6    \\\n",
       "0  [-0.55358464, 0.2415753, -0.39307374, 1.591386...   \n",
       "1  [-0.9703825, 1.7481861, -1.1898727, -1.808933,...   \n",
       "\n",
       "                                                 7    \\\n",
       "0  [0.73056346, 1.5591023, -0.77380407, -0.267105...   \n",
       "1  [0.72331065, -0.56463075, -0.323733, -1.384814...   \n",
       "\n",
       "                                                 8    \\\n",
       "0  [-0.55358464, 0.2415753, -0.39307374, 1.591386...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 9    ...  \\\n",
       "0  [-0.009174255, 1.0757917, 1.0208045, -0.333414...  ...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...  ...   \n",
       "\n",
       "                                                 393  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 394  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 395  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 396  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 397  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 398  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 399  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 400  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 401  \\\n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...   \n",
       "\n",
       "                                                 402  \n",
       "0  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...  \n",
       "1  [-1.7984205, -1.0761415, 0.80871004, 0.1154940...  \n",
       "\n",
       "[2 rows x 403 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "In every cell we have 50-dimensional vector of an embedded word. Let's check: the first vector in the first line should be\n",
    "the word 'категорически'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0e83793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.iloc[0, 0] == w2v_model.wv['категорически']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf7aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Good, everything is allright\n",
    "Lets take embedding of a sentnence as weighted mean of every word in the sentence.\n",
    "I am going to use idf of a word divided by sum of idf's in this sentence. It allows us to give more weight to the rarest words.\n",
    "They are what makes each review unique and defines it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb3c0361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483bb4abc1a34e4795f8d908c0f1bd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcec967b93c547a5be2d1a782faf4cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create mapping: (token, idf вес)\n",
    "word2idf = {}\n",
    "mean_idf_weight, n = 0, 0\n",
    "for word, idx in tqdm(tf_idf.vocabulary_.items()):\n",
    "    word2idf[word] = tf_idf.idf_[idx]\n",
    "    mean_idf_weight += tf_idf.idf_[idx]\n",
    "    n += 1\n",
    "mean_idf_weight /= n\n",
    "\n",
    "X_tr_emb = []\n",
    "emb_size = len(w2v_model.wv['не'])\n",
    "for text in tqdm(train_lemmatized_descriptions):\n",
    "    res = np.zeros(emb_size)\n",
    "    denominator = 1e-20\n",
    "    for token in text:\n",
    "        idf_weight = word2idf.get(token, mean_idf_weight)\n",
    "        denominator += idf_weight\n",
    "        try:\n",
    "            res += w2v_model.wv[token] * idf_weight\n",
    "        except:\n",
    "            res += np.zeros(emb_size)\n",
    "    res /= denominator\n",
    "    X_tr_emb.append(list(res))\n",
    "X_tr_emb = np.array(X_tr_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1de6685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad258827f8044bb484c78232cb7140eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21747 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_te_emb = []\n",
    "emb_size = len(w2v_model.wv['не'])\n",
    "for text in tqdm(test_lemmatized_descriptions):\n",
    "    res = np.zeros(emb_size)\n",
    "    denominator = 1e-20\n",
    "    for token in text:\n",
    "        idf_weight = word2idf.get(token, mean_idf_weight)\n",
    "        denominator += idf_weight\n",
    "        try:\n",
    "            res += w2v_model.wv[token] * idf_weight\n",
    "        except:\n",
    "            res += np.zeros(emb_size)\n",
    "    res /= denominator\n",
    "    X_te_emb.append(list(res))\n",
    "X_te_emb = np.array(X_te_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1a37a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.62232392 -0.0476773  -0.67265277 -0.11154408  0.36035627 -0.85433612\n",
      " -0.7293352  -0.06254526 -0.75194891  0.52786277 -0.24420437 -0.85110287\n",
      "  0.63250629  0.67324755 -0.71638756  0.23487896 -0.58970651  0.81449972\n",
      "  0.06440735  0.59710945 -0.09617775  0.20778339 -0.21112607 -0.47319356\n",
      "  0.49213558 -1.40247931 -0.41586154  0.21096614 -0.32726582 -0.14366438\n",
      "  0.14812559 -0.62541031 -0.14207857 -0.09900206 -0.7507755  -0.13075403\n",
      "  0.13117144 -0.54457227 -0.01814818 -0.6151011  -0.09563059  0.02106113\n",
      "  0.16496755 -0.18688816 -0.60734472 -0.4024456   0.32109422 -0.87516085\n",
      "  0.46509093  0.59177074]\n",
      "(22892, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr_emb[0, :])\n",
    "print(np.shape(X_tr_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here we got the embedding for the first review.\n",
    "Nice, now we have fully embedded dataset, we can run the ml model. We are going to use logistic regression as we need to get\n",
    "the best score according to ROS-AUC score and we believe that it describes our data structure rather well. Before training we\n",
    "need to optimize the hyperparameter C and l1_ratio through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bc7616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 8.6867\n",
      "Function value obtained: -0.7060\n",
      "Current minimum: -0.7060\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 7.6483\n",
      "Function value obtained: -0.7068\n",
      "Current minimum: -0.7068\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 8.0521\n",
      "Function value obtained: -0.7135\n",
      "Current minimum: -0.7135\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 7.7719\n",
      "Function value obtained: -0.7133\n",
      "Current minimum: -0.7135\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 7.3245\n",
      "Function value obtained: -0.7191\n",
      "Current minimum: -0.7191\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 6.8187\n",
      "Function value obtained: -0.7209\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 8.2669\n",
      "Function value obtained: -0.7049\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 8.1698\n",
      "Function value obtained: -0.7036\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 8.1524\n",
      "Function value obtained: -0.7099\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 6.6902\n",
      "Function value obtained: -0.7176\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 6.3423\n",
      "Function value obtained: -0.7091\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 6.5026\n",
      "Function value obtained: -0.7091\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 6.4247\n",
      "Function value obtained: -0.7091\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.3862\n",
      "Function value obtained: -0.7205\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.8281\n",
      "Function value obtained: -0.7186\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.8211\n",
      "Function value obtained: -0.7172\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.1283\n",
      "Function value obtained: -0.7159\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.5892\n",
      "Function value obtained: -0.7079\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.3787\n",
      "Function value obtained: -0.7117\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.8775\n",
      "Function value obtained: -0.7187\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.7631\n",
      "Function value obtained: -0.7192\n",
      "Current minimum: -0.7209\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.0107\n",
      "Function value obtained: -0.7218\n",
      "Current minimum: -0.7218\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 9.3176\n",
      "Function value obtained: -0.7059\n",
      "Current minimum: -0.7218\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.3765\n",
      "Function value obtained: -0.7144\n",
      "Current minimum: -0.7218\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.4891\n",
      "Function value obtained: -0.7109\n",
      "Current minimum: -0.7218\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.1943\n",
      "Function value obtained: -0.7223\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.4404\n",
      "Function value obtained: -0.7075\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.0647\n",
      "Function value obtained: -0.7218\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.9913\n",
      "Function value obtained: -0.7168\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.1118\n",
      "Function value obtained: -0.7216\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.4226\n",
      "Function value obtained: -0.7123\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.3877\n",
      "Function value obtained: -0.7091\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.0937\n",
      "Function value obtained: -0.7196\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.1881\n",
      "Function value obtained: -0.7219\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.6139\n",
      "Function value obtained: -0.7057\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.7135\n",
      "Function value obtained: -0.7188\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.5251\n",
      "Function value obtained: -0.7041\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 8.6046\n",
      "Function value obtained: -0.7082\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.6374\n",
      "Function value obtained: -0.7219\n",
      "Current minimum: -0.7223\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 7.3022\n",
      "Function value obtained: -0.7220\n",
      "Current minimum: -0.7223\n",
      "Best AUC-ROC: 0.722\n",
      "Best Parameters: [0.8249524673249442, 0.001]\n"
     ]
    }
   ],
   "source": [
    "# define search space\n",
    "search_space = list()\n",
    "search_space.append(Real(0.1, 10, name='C'))\n",
    "search_space.append(Real(10**(-3), 1, name='l1_ratio'))\n",
    "\n",
    "\n",
    "# define the function needed to optimize\n",
    "@use_named_args(search_space)\n",
    "def cross_val_mean(**param):\n",
    "    log_reg = LogisticRegression(**param, n_jobs=-1)\n",
    "    acc = -np.mean(cross_val_score(estimator=log_reg, X=X_tr, y=y_train, scoring=make_scorer(roc_auc_score), cv=5))\n",
    "    return acc\n",
    "\n",
    "\n",
    "# perform optimization\n",
    "result = gp_minimize(\n",
    "    func=cross_val_mean,\n",
    "    dimensions=search_space,\n",
    "    n_calls=40,\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print('Best AUC-ROC: %.3f' % (-result.fun))\n",
    "print('Best Parameters: %s' % (result.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad107e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "We got our best hyperparameters, now we can fit the model and give predictions for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d90423c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our best model on the whole train dataframe\n",
    "C, l1_ratio = result.x\n",
    "log_reg = LogisticRegression(C=C, l1_ratio=l1_ratio, n_jobs=-1)\n",
    "\n",
    "log_reg.fit(X=X_tr_emb, y=y_train)\n",
    "pred_test = log_reg.predict_proba(X=X_te_emb)[:, 1]\n",
    "submission = pd.DataFrame()\n",
    "submission[\"id\"] = test_ids\n",
    "submission[\"sentiment\"] = pred_test\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2694fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
